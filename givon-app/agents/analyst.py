"""
×¡×•×›×Ÿ × ×™×ª×•×— â€” Givon Defense Intelligence Analyst
×œ×•×§×— ×¤×¨×™×˜×™× ×’×•×œ××™×™× ××”×¡×¨×™×§×” ×•×× ×ª×— ××•×ª× ×“×¨×š Claude API
"""

import anthropic
import json
import time
import hashlib
import os
from datetime import datetime
from typing import Optional

# â”€â”€â”€ Hash Memory â€” ×–×™×›×¨×•×Ÿ ×œ×—×™×¡×›×•×Ÿ ×‘×¢×œ×•×ª API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HASH_CACHE_FILE = "analyzed_hashes.json"

def _make_hash(item: dict) -> str:
    key = f"{item.get('title', '')}|{item.get('source', '')}|{item.get('url', '')}"
    return hashlib.md5(key.encode("utf-8")).hexdigest()

def load_hash_cache() -> set:
    if os.path.exists(HASH_CACHE_FILE):
        try:
            with open(HASH_CACHE_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f).get("hashes", []))
        except (json.JSONDecodeError, KeyError):
            return set()
    return set()

def save_hash_cache(cache: set):
    with open(HASH_CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump({"hashes": list(cache), "count": len(cache),
                   "last_updated": datetime.now().isoformat()}, f, ensure_ascii=False, indent=2)

# â”€â”€â”€ ×¤×¨×•×¤×™×œ ×’×‘×¢×•×Ÿ (system prompt) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

GIVON_PROFILE = """
××ª×” ×× ×œ×™×¡×˜ ××•×“×™×¢×™×Ÿ ××¡×˜×¨×˜×’×™ ×©×œ Givon Defense (×’×‘×¢×•×Ÿ ×‘×™×˜×—×•×Ÿ).

×¤×¨×•×¤×™×œ ×’×‘×¢×•×Ÿ:
- ×—×‘×¨×” ×™×©×¨××œ×™×ª: Next-Gen Decentralized National Security Prime
- ××™×™×¡×“×™×: ××œ×´× (××™×œ.) ××‘×™ ×’×™×œ (×™×•×´×¨), ×¡××´×œ (××™×œ.) ×™×•× ×ª×Ÿ ×¨×•× (×× ×›×´×œ)
- ××•×“×œ: ××–×”×” ×¤×¢×¨×™× ××‘×¦×¢×™×™×, ×‘×•× ×” ventures, ×¤×•×¨×¡ ×ª×•×š 12-24 ×—×•×“×©

×¤×•×¨×˜×¤×•×œ×™×• Ventures:
- Guardian Angel (TRL 7) â€” AI mesh covert ×œ××‘×˜×—×ª ×’×‘×•×œ×•×ª ×•×ª×©×ª×™×•×ª
- Sky Fort (TRL 5) â€” Counter-UAS point defense ××•×˜×•× ×•××™, × ×’×“ × ×—×™×œ×™×
- Aerosentry (TRL 7) â€” Counter-UAS long-range, ××ª××—×” ×‘-Shahed/Geran
- Daya IRIS-20 (TRL 5) â€” Aerial ISR, ×›×™×¡×•×™ 100 ×§×´×, ×¢×œ×•×ª × ××•×›×” ×‘-80-90%
- DFM Power (TRL 9) â€” Nano-grid ××•×“×•×œ×¨×™, 300 ×§×´×’, APMS AI-driven
- Crebain (TRL 5) â€” AI-driven decentralized swarm, hardware-agnostic

Solutions Companies:
- D-COE (×”×“×¨×›×” ×•×¡×™××•×œ×¦×™×” ×œ×“×¨×•× ×™×)
- GuaRdF (RF sensing ×•×¢×§×™×‘×” ×”××•× ×™×ª)
- iCit (Vision Agents ×œ×”×’× ×”)
- D-Fence (×¤×ª×¨×•× ×•×ª ×—×™×™×©× ×™× ××ª×§×“××™×)
- Top I Vision (×¤×œ×˜×¤×•×¨××•×ª ××•×•×™×¨×™×•×ª)
- Mokoushla (×¨×•×‘×•×˜×™×§×” ×§×¨×§×¢×™×ª ××•×›×—×ª ×‘×©×“×”)
- Visual Layer (× ×™×”×•×œ × ×ª×•× ×™× ×•×™×–×•××œ×™×™× AI)
- Cyberbee (× ×™×•×•×˜ GNSS-denied ×¢× AI Vision)
- Elite Minds (×¤×œ×˜×¤×•×¨××ª ×¡×™×™×‘×¨)

×©×•×•×§×™ ×™×¢×“: US DoD (DIU/AFWERX/DARPA/SOCOM), NATO/DIANA, EU (EDF/Horizon), ×××¤×´×ª/××œ××´×‘, UK MOD, ××–×¨×— ××™×¨×•×¤×”

×ª×—×•××™ ×œ×™×‘×”: Counter-UAS, Swarm AI, Aerial ISR, Tactical Energy, Border Security, Vision AI, Robotics, RF/Cyber
"""

ANALYST_INSTRUCTIONS = """
×§×™×‘×œ×ª ×¤×¨×™×˜ ×’×•×œ××™ ×©× ×¡×¨×§ ×××§×•×¨ ××•×“×™×¢×™×Ÿ. ×”××©×™××” ×©×œ×š:

1. ×§×¨× ××ª ×”×¤×¨×™×˜
2. ×”×—×œ×˜ ×× ×”×•× ×¨×œ×•×•× ×˜×™ ×œ×’×‘×¢×•×Ÿ (fitScore >= 40 = ×¨×œ×•×•× ×˜×™)
3. ×× ×¨×œ×•×•× ×˜×™ â€” ×”×¤×§ JSON ××•×‘× ×”

×›×œ×œ×™ × ×™×ª×•×—:
- fitScore: 0-100 ×œ×¤×™ ×”×ª×××” ×œ×¤×•×¨×˜×¤×•×œ×™×• ×’×‘×¢×•×Ÿ (TRL, domain, market access, timing)
- urgency: critical (×“×“×œ×™×™×Ÿ <30 ×™×•× / ×”×–×“×× ×•×ª ×—×“-×¤×¢××™×ª), high, medium, low
- category: contracts / partners / investors / grants / ventures / competitors
- whyRelevant: 2-3 ××©×¤×˜×™× ×‘×¢×‘×¨×™×ª â€” ×¦×™×™×Ÿ ×©××•×ª ×¡×¤×¦×™×¤×™×™× ××”×¤×•×¨×˜×¤×•×œ×™×• ×•×œ××” ×–×• ×”×–×“×× ×•×ª ×××™×ª×™×ª

×¤×•×¨××˜ ×ª×’×•×‘×” â€” JSON ×‘×œ×‘×“, ×œ×œ× markdown:
{
  "relevant": true/false,
  "title": "×›×•×ª×¨×ª ×§×¦×¨×” ×‘×× ×’×œ×™×ª",
  "org": "×©× ×”×’×•×£",
  "country": "××“×™× ×” ×‘×¢×‘×¨×™×ª",
  "flag": "×××•×’×³×™ ×“×’×œ",
  "category": "contracts/partners/investors/grants/ventures/competitors",
  "domain": "Counter-UAS / Swarm AI / Aerial ISR / Tactical Energy / ××‘×˜×—×ª ×’×‘×•×œ×•×ª / Cyber-RF / ×¨×•×‘×•×˜×™×§×” / Vision AI / Industry / News / Analysis",
  "budget": "×¡×›×•× + ××˜×‘×¢ (×× ×™×“×•×¢)",
  "deadline": "DD.MM.YYYY (×× ×™×“×•×¢)",
  "fitScore": 0-100,
  "action": "×§×‘×œ×Ÿ ×¨××©×™ / ×§×‘×œ×Ÿ ××©× ×” / ×©×•×ª×£ / ×—×§×•×¨ Venture / ×¢×§×•×‘ / ×”×ª×¢×œ×",
  "urgency": "critical/high/medium/low",
  "tag": "OTA/SBIR/EDF/NATO/News/Industry/Analysis/etc",
  "whyRelevant": "2-3 ××©×¤×˜×™× ×¢×‘×¨×™×ª ×¢× ×©××•×ª ×¡×¤×¦×™×¤×™×™× ××”×¤×•×¨×˜×¤×•×œ×™×•",
  "summary": "××©×¤×˜ ××—×“ ×¢×‘×¨×™×ª â€” ×ª××¦×™×ª ×”×¤×¨×™×˜",
  "url": "URL ×”××§×•×¨"
}

×× relevant: false â€” ×”×—×–×¨ ×¨×§: {"relevant": false}
"""


# â”€â”€â”€ Analyst â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def analyze_item(client: anthropic.Anthropic, item: dict) -> Optional[dict]:
    """×× ×ª×— ×¤×¨×™×˜ ×‘×•×“×“ ×“×¨×š Claude API"""
    try:
        # ×‘× ×” ×ª×™××•×¨ ×”×¤×¨×™×˜
        item_text = f"""
××§×•×¨: {item.get('source', '')}
×›×•×ª×¨×ª: {item.get('title', '')}
×’×•×£: {item.get('org', '')}
××“×™× ×”: {item.get('country', '')}
×ª×™××•×¨: {item.get('description', '')}
×“×“×œ×™×™×Ÿ: {item.get('deadline', '×œ× ×™×“×•×¢')}
×¤×•×¨×¡×: {item.get('posted', '')}
×¡×•×’: {item.get('type', '')}
URL: {item.get('url', '')}
×‘×“×™×§×” ×™×“× ×™×ª × ×“×¨×©×ª: {item.get('manual_check', False)}
"""

        response = client.messages.create(
            model="claude-opus-4-6",
            max_tokens=600,
            system=GIVON_PROFILE + "\n\n" + ANALYST_INSTRUCTIONS,
            messages=[{"role": "user", "content": item_text}]
        )

        text = response.content[0].text.strip()
        # × ×§×” markdown ×× ×™×©
        text = text.replace("```json", "").replace("```", "").strip()
        result = json.loads(text)

        if not result.get("relevant", False):
            return None

        # ×”×•×¡×£ metadata
        result["id"] = f"{item.get('source', 'unknown')}_{hash(item.get('title', ''))}"
        result["scanned_at"] = datetime.now().isoformat()
        result["source_name"] = item.get("source", "")
        result["source_tier"] = item.get("source_tier", 2)
        result["bookmarked"] = False
        result["status"] = "×¤×ª×•×—"
        result["assignee"] = None

        return result

    except json.JSONDecodeError as e:
        print(f"  JSON parse error: {e}")
        return None
    except Exception as e:
        print(f"  Error analyzing item: {e}")
        return None


def run_analysis(raw_items: list, api_key: str, batch_size: int = 5) -> list:
    """
    ×× ×ª×— ××ª ×›×œ ×”×¤×¨×™×˜×™× ×”×’×•×œ××™×™×.
    ××“×œ×’ ×¢×œ ×¤×¨×™×˜×™× ×©×›×‘×¨ × ×•×ª×—×• â€” ×—×™×¡×›×•×Ÿ 80-90% ×‘×¢×œ×•×ª API ×”×—×œ ××¨×™×¦×” ×©× ×™×™×”.
    """
    client = anthropic.Anthropic(api_key=api_key)

    # â”€â”€ ×˜×¢×™× ×ª Hash cache â”€â”€
    hash_cache = load_hash_cache()
    cache_hits = 0

    analyzed = []
    skipped = 0
    errors = 0

    print(f"\n×× ×ª×— {len(raw_items)} ×¤×¨×™×˜×™×...")
    print(f"Hash cache: {len(hash_cache)} ×¤×¨×™×˜×™× ×™×“×•×¢×™× ××¨×™×¦×•×ª ×§×•×“××•×ª")
    print("=" * 50)

    for i, item in enumerate(raw_items):
        title = item.get("title", "")[:60]
        print(f"[{i+1}/{len(raw_items)}] {title}...")

        # ×“×œ×’ ×¢×œ LinkedIn manual items â€” ×™×˜×•×¤×œ×• ×‘× ×¤×¨×“
        if item.get("manual_check"):
            print(f"  â†’ LinkedIn manual, ××“×œ×’")
            skipped += 1
            continue

        # â”€â”€ ×‘×“×™×§×ª Hash â€” ×”×× ×›×‘×¨ × ×•×ª×—? â”€â”€
        item_hash = _make_hash(item)
        if item_hash in hash_cache:
            print(f"  â†’ âš¡ cache hit, ××“×œ×’")
            cache_hits += 1
            skipped += 1
            continue

        result = analyze_item(client, item)

        # ×©××•×¨ hash ×‘×›×œ ××§×¨×” â€” ×œ× ×œ× ×ª×— ×©×•×‘ ×’× ×× ×œ× ×¨×œ×•×•× ×˜×™
        hash_cache.add(item_hash)

        if result:
            analyzed.append(result)
            print(f"  â†’ âœ… fitScore: {result.get('fitScore', 0)} | {result.get('category', '')} | {result.get('domain', '')}")
        else:
            skipped += 1
            print(f"  â†’ â­ ×œ× ×¨×œ×•×•× ×˜×™")

        # ×”×©×”×™×” ×œ×× ×™×¢×ª rate limiting
        if (i + 1) % batch_size == 0:
            time.sleep(2)
        else:
            time.sleep(0.5)

    # â”€â”€ ×©××™×¨×ª cache ××¢×•×“×›×Ÿ â”€â”€
    save_hash_cache(hash_cache)

    print(f"\n{'='*50}")
    print(f"×¨×œ×•×•× ×˜×™×™×: {len(analyzed)} | ×“×•×œ×’×•: {skipped} | Cache hits: {cache_hits} | ×©×’×™××•×ª: {errors}")
    if cache_hits > 0:
        print(f"ğŸ’° ×—×™×¡×›×•×Ÿ ××©×•×¢×¨: ~${cache_hits * 0.003:.2f} ×‘×¢×œ×•×ª API")
    return analyzed


def merge_with_existing(new_items: list, existing_path: str = "../givon-app/src/opportunities.json") -> list:
    """
    ×××–×’ ×¢× ×”× ×ª×•× ×™× ×”×§×™×™××™× â€” ×©×•××¨ ×¤×¨×™×˜×™× ×©×¢×“×™×™×Ÿ ×¨×œ×•×•× ×˜×™×™×,
    ××•×¡×™×£ ×—×“×©×™×, ××¡×™×¨ ×™×©× ×™× (××¢×œ 60 ×™×•×)
    """
    try:
        with open(existing_path, "r", encoding="utf-8") as f:
            existing = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        existing = []

    # ×‘× ×” lookup ×œ×¤×™ ID
    existing_ids = {item.get("id", ""): item for item in existing}
    new_ids = {item.get("id", ""): item for item in new_items}

    # ×©××•×¨ ×¤×¨×™×˜×™× ×§×™×™××™× ×©×œ× × ××—×§×• ×•×œ× ×¤×’×•
    cutoff = datetime.now().isoformat()[:10]  # ×ª××¨×™×š ×”×™×•×
    kept = []
    for item_id, item in existing_ids.items():
        deadline = item.get("deadline", "")
        # ×©××•×¨ ××: ×—×“×© (××•×¤×™×¢ ×‘×¡×¨×™×§×”), ××• ×¡×˜×˜×•×¡ ×¤×¢×™×œ, ××• ××™×Ÿ ×“×“×œ×™×™×Ÿ
        if item_id in new_ids or item.get("status") in ("×‘×‘×“×™×§×”", "×”×•×’×©") or not deadline:
            kept.append(item)

    # ×”×•×¡×£ ×¤×¨×™×˜×™× ×—×“×©×™× ×©×œ× ×”×™×• ×§×•×“×
    added = [item for item in new_items if item.get("id") not in existing_ids]

    merged = kept + added
    print(f"××™×–×•×’: {len(kept)} ×§×™×™××™× + {len(added)} ×—×“×©×™× = {len(merged)} ×¡×”×´×›")
    return merged


def sort_by_priority(items: list) -> dict:
    """
    ×××™×™×Ÿ ×•×××¨×’×Ÿ ×œ×¤×™ ×§×˜×’×•×¨×™×” ×•-fitScore
    ××—×–×™×¨ dict ×œ×¤×™ ×§×˜×’×•×¨×™×•×ª â€” ××•×›×Ÿ ×œ×¤×•×¨×˜×œ
    """
    categories = {
        "contracts": [],
        "partners": [],
        "investors": [],
        "grants": [],
        "ventures": [],
        "competitors": [],
    }

    for item in items:
        cat = item.get("category", "contracts")
        if cat in categories:
            categories[cat].append(item)

    # ××™×™×Ÿ ×›×œ ×§×˜×’×•×¨×™×” ×œ×¤×™ fitScore
    for cat in categories:
        categories[cat].sort(key=lambda x: x.get("fitScore", 0), reverse=True)

    return categories


if __name__ == "__main__":
    import os
    import sys

    # ×§×‘×œ API key
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        print("×©×’×™××”: ANTHROPIC_API_KEY ×œ× ××•×’×“×¨")
        print("×”×¨×¥: export ANTHROPIC_API_KEY='your-key-here'")
        sys.exit(1)

    # ×˜×¢×Ÿ × ×ª×•× ×™× ×’×•×œ××™×™×
    try:
        with open("raw_scan.json", "r", encoding="utf-8") as f:
            raw_items = json.load(f)
        print(f"× ×˜×¢× ×• {len(raw_items)} ×¤×¨×™×˜×™× ×’×•×œ××™×™×")
    except FileNotFoundError:
        print("×©×’×™××”: raw_scan.json ×œ× × ××¦×. ×”×¨×¥ scanner.py ×§×•×“×.")
        sys.exit(1)

    # × ×ª×—
    analyzed = run_analysis(raw_items, api_key)

    # ××–×’ ×¢× ×§×™×™××™×
    merged = merge_with_existing(analyzed)

    # ××¨×’×Ÿ ×œ×¤×™ ×§×˜×’×•×¨×™×•×ª
    organized = sort_by_priority(merged)

    # ×©××•×¨
    output_path = "../givon-app/src/opportunities.json"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(organized, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… × ×©××¨ ×œ-{output_path}")
    for cat, items in organized.items():
        print(f"  {cat}: {len(items)} ×¤×¨×™×˜×™×")
